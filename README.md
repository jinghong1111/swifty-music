
# Taylor Swift Music VAE Deep Learning Project # 

### Music Variational Autoencoder (Music VAE) ##
# Music Variational Autoencoder (Music VAE)

The Music Variational Autoencoder (Music VAE) is a generative model designed to learn meaningful representations of musical data in a latent space. This README provides an overview of the Music VAE model architecture and its training method.

## Introduction

The Music VAE is a type of Variational Autoencoder (VAE) tailored specifically for music data. It aims to capture the essential features of musical pieces in a lower-dimensional latent space. By learning a structured representation of music, the model can generate new musical compositions, perform music interpolation, and provide insights into the underlying patterns within music.

## Model Architecture

The Music VAE consists of two main components: an encoder and a decoder.

### Encoder
The encoder takes a musical sequence as input and maps it into the latent space. It comprises a series of neural network layers that transform the input into the parameters of a multivariate Gaussian distribution. These parameters represent the mean and log variance of the latent space.

### Decoder
The decoder generates musical sequences from latent space vectors sampled from the encoder. It reconstructs the original musical data by transforming the latent vectors through a series of neural network layers, producing an output sequence that resembles the input data.

## Training Method

The training of the Music VAE involves two primary objectives: data reconstruction and regularization through the Kullback-Leibler (KL) divergence.

1. **Data Reconstruction Loss**: The model minimizes the difference between the original input musical sequence and the sequence generated by the decoder. This is achieved through a binary cross-entropy loss, which measures the similarity between the input and the generated output.

2. **KL Divergence Loss**: The Music VAE employs the KL divergence to ensure that the latent space distribution learned by the encoder is close to a prior distribution (usually a standard Gaussian). This step encourages the model to learn a meaningful and well-structured latent representation.

The total loss for training is the combination of the data reconstruction loss and the KL divergence loss. By optimizing this loss function, the model learns to efficiently encode musical data into a compact latent space.

## Usage

To use the Music VAE model, follow these steps:

1. **Data Preparation**: Preprocess your musical data into appropriate input sequences.
2. **Model Initialization**: Create an instance of the Music VAE model.
3. **Training**: Feed your training data into the model using the `train_step` function provided. Adjust hyperparameters and optimizer settings as needed.
4. **Generation**: Once trained, you can sample from the latent space to generate new musical sequences using the decoder.

Through using WAV files from selected parts of Taylor Swift's music, the model can compose original sound tracks mimicking Taylor Swift's music style.

## To copy and recreate this project, please use the command: pip3 install requirements.txt OR run the installation commands in the first cell of ts_wav.ipynb ##